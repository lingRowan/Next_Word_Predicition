{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio\n",
        "import gradio as gr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jfQmgISGsiY-",
        "outputId": "df5c93c8-eb91-4da8-dc02-c09535008ab3"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.10/dist-packages (5.5.0)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2.1)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.115.4)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio) (0.4.0)\n",
            "Requirement already satisfied: gradio-client==1.4.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.4.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.27.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.25.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.26.2)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.10)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.1)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (10.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.9.2)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart==0.0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.0.12)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.7.2)\n",
            "Requirement already satisfied: safehttpx<1.0,>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.1.1)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.41.2)\n",
            "Requirement already satisfied: tomlkit==0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.0)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.5)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.32.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.4.2->gradio) (2024.10.0)\n",
            "Requirement already satisfied: websockets<13.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.4.2->gradio) (12.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (3.16.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (4.66.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.23.4)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (2.2.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "6pL7HtUjp9_J"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import pickle\n",
        "import gradio as gr"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://gist.githubusercontent.com/Momnadar1/8805a6d53e92d6be17b9837711a5931a/raw/adc9cc97efc92232f01cbb6a1b13e8123d9f8f8d/shakepeare_s_plays.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pqgu60uIqCDT",
        "outputId": "75b828d6-dd0d-4521-acc3-511da74c7d51"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-11-05 14:44:27--  https://gist.githubusercontent.com/Momnadar1/8805a6d53e92d6be17b9837711a5931a/raw/adc9cc97efc92232f01cbb6a1b13e8123d9f8f8d/shakepeare_s_plays.txt\n",
            "Resolving gist.githubusercontent.com (gist.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to gist.githubusercontent.com (gist.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5583374 (5.3M) [text/plain]\n",
            "Saving to: ‘shakepeare_s_plays.txt.1’\n",
            "\n",
            "shakepeare_s_plays. 100%[===================>]   5.32M  --.-KB/s    in 0.09s   \n",
            "\n",
            "2024-11-05 14:44:27 (60.3 MB/s) - ‘shakepeare_s_plays.txt.1’ saved [5583374/5583374]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('shakepeare_s_plays.txt', 'r') as f:\n",
        "    text = f.read()\n",
        "# print from the data\n",
        "print(text[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MhllDMh6qIDX",
        "outputId": "de1991aa-98c8-484a-e1ce-690467847ddf"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Hamlet\n",
            "\n",
            "ACT I\n",
            "SCENE I. Elsinore. A platform before the castle.\n",
            "\n",
            "    FRANCISCO at his post. Enter t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# data preprocessing\n",
        "# mapping each character with it's index\n",
        "chars = sorted(list(set(text))) # chars represent special characters\n",
        "vocab_size = len(chars) # getting the len of distinct characters\n",
        "print(''.join(chars), vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72_T-0w5qK58",
        "outputId": "3b07c88f-c864-48b2-ac2c-f78d44737448"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t\n",
            " !#$&'(),-.0123456789:;=>?ABCDEFGHIJKLMNOPQRSTUVWXYZ[]abcdefghijklmnopqrstuvwxyzÀè—‘’“”… 90\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# encoding and decoding function for training and testing the machine learning models\n",
        "str_to_int = {char:i for i, char in enumerate(chars)}\n",
        "int_to_str = {i:char for i, char in enumerate(chars)}\n",
        "\n",
        "encode = lambda string: [str_to_int[s] for s in string]\n",
        "decode = lambda indexes: [int_to_str[i] for i in indexes]\n",
        "\n",
        "''.join(decode(encode('hi work!!')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Yq1DWcsHqNT2",
        "outputId": "43a2b4cb-e643-4a02-c8b0-51e1444532a7"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'hi work!!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "str_to_int # mapping each character with it's index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9rJPVKcqQOm",
        "outputId": "bd6e0708-0132-4e50-dd07-3616f9d1bde6"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'\\t': 0,\n",
              " '\\n': 1,\n",
              " ' ': 2,\n",
              " '!': 3,\n",
              " '#': 4,\n",
              " '$': 5,\n",
              " '&': 6,\n",
              " \"'\": 7,\n",
              " '(': 8,\n",
              " ')': 9,\n",
              " ',': 10,\n",
              " '-': 11,\n",
              " '.': 12,\n",
              " '0': 13,\n",
              " '1': 14,\n",
              " '2': 15,\n",
              " '3': 16,\n",
              " '4': 17,\n",
              " '5': 18,\n",
              " '6': 19,\n",
              " '7': 20,\n",
              " '8': 21,\n",
              " '9': 22,\n",
              " ':': 23,\n",
              " ';': 24,\n",
              " '=': 25,\n",
              " '>': 26,\n",
              " '?': 27,\n",
              " 'A': 28,\n",
              " 'B': 29,\n",
              " 'C': 30,\n",
              " 'D': 31,\n",
              " 'E': 32,\n",
              " 'F': 33,\n",
              " 'G': 34,\n",
              " 'H': 35,\n",
              " 'I': 36,\n",
              " 'J': 37,\n",
              " 'K': 38,\n",
              " 'L': 39,\n",
              " 'M': 40,\n",
              " 'N': 41,\n",
              " 'O': 42,\n",
              " 'P': 43,\n",
              " 'Q': 44,\n",
              " 'R': 45,\n",
              " 'S': 46,\n",
              " 'T': 47,\n",
              " 'U': 48,\n",
              " 'V': 49,\n",
              " 'W': 50,\n",
              " 'X': 51,\n",
              " 'Y': 52,\n",
              " 'Z': 53,\n",
              " '[': 54,\n",
              " ']': 55,\n",
              " 'a': 56,\n",
              " 'b': 57,\n",
              " 'c': 58,\n",
              " 'd': 59,\n",
              " 'e': 60,\n",
              " 'f': 61,\n",
              " 'g': 62,\n",
              " 'h': 63,\n",
              " 'i': 64,\n",
              " 'j': 65,\n",
              " 'k': 66,\n",
              " 'l': 67,\n",
              " 'm': 68,\n",
              " 'n': 69,\n",
              " 'o': 70,\n",
              " 'p': 71,\n",
              " 'q': 72,\n",
              " 'r': 73,\n",
              " 's': 74,\n",
              " 't': 75,\n",
              " 'u': 76,\n",
              " 'v': 77,\n",
              " 'w': 78,\n",
              " 'x': 79,\n",
              " 'y': 80,\n",
              " 'z': 81,\n",
              " 'À': 82,\n",
              " 'è': 83,\n",
              " '—': 84,\n",
              " '‘': 85,\n",
              " '’': 86,\n",
              " '“': 87,\n",
              " '”': 88,\n",
              " '…': 89}"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "data[:100], data.shape, data.dtype"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JevwgbDhqSg9",
        "outputId": "092f0bee-e2ee-43fc-94fc-60e2d905de06"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([ 4,  2, 35, 56, 68, 67, 60, 75,  1,  1, 28, 30, 47,  2, 36,  1, 46, 30,\n",
              "         32, 41, 32,  2, 36, 12,  2, 32, 67, 74, 64, 69, 70, 73, 60, 12,  2, 28,\n",
              "          2, 71, 67, 56, 75, 61, 70, 73, 68,  2, 57, 60, 61, 70, 73, 60,  2, 75,\n",
              "         63, 60,  2, 58, 56, 74, 75, 67, 60, 12,  1,  1,  2,  2,  2,  2, 33, 45,\n",
              "         28, 41, 30, 36, 46, 30, 42,  2, 56, 75,  2, 63, 64, 74,  2, 71, 70, 74,\n",
              "         75, 12,  2, 32, 69, 75, 60, 73,  2, 75]),\n",
              " torch.Size([5580526]),\n",
              " torch.int64)"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "splitter = int(0.9*len(data)) # calculate the number which represent 90% of the data\n",
        "\n",
        "train, test = data[:splitter], data[splitter:] #train from 0 to 90 and the test from 90 to 100"
      ],
      "metadata": {
        "id": "CtTDm3FKqUv5"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_emb = 64\n",
        "block_size = 32\n",
        "# head_size = 4\n",
        "n_x = 4\n",
        "num_heads = 4\n",
        "eval_iteration = 250\n",
        "max_iters = 5000\n",
        "batch_size = 32\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "smQNhwI1qXmP"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%%\n",
        "class Head(nn.Module):\n",
        "    \"\"\"\n",
        "    one head in self attention\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_emb, head_size)\n",
        "        self.query = nn.Linear(n_emb, head_size)\n",
        "        self.value = nn.Linear(n_emb, head_size)\n",
        "        self.dropout = nn.Dropout(0.0)\n",
        "\n",
        "        # tril: lower-triangular\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch, blocks, X = x.shape\n",
        "        query = self.query(x) # batch, block_size, X -- shape\n",
        "        key = self.key(x) # batch, block_size, X -- shape\n",
        "        weight = query @ key.transpose(-2, -1)  * X ** -0.5 # batch, block_size, X @ batch, X, blocl_size ---> batch, block_size, block_size\n",
        "        weight = weight.masked_fill(self.tril[:blocks, :blocks] == 0,float('-inf'))\n",
        "        weight = F.softmax(weight, dim=-1)\n",
        "        weight = self.dropout(weight)\n",
        "        out = weight @  self.value(x)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    multi head in self attention\n",
        "    \"\"\"\n",
        "    # nnum_head = 6\n",
        "    # head_size\n",
        "    def __init__(self, head_size, num_heads):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.layer = nn.Linear(n_emb, n_emb)\n",
        "        self.dropout = nn.Dropout(0.0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        return self.dropout(self.layer(out))\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "\n",
        "    def __init__(self, n_emb):\n",
        "        super().__init__()\n",
        "        self.dff = nn.Sequential(\n",
        "            nn.Linear(n_emb, n_emb*4),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4*n_emb, n_emb),\n",
        "            nn.Dropout(0.0)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.dff(x)\n",
        "\n",
        "class BlockSeq(nn.Module):\n",
        "    def __init__(self, n_emb, num_heads):\n",
        "        super().__init__()\n",
        "        head_size = int(n_emb / num_heads)\n",
        "        self.mh_att = MultiHeadAttention(head_size, num_heads)\n",
        "        self.ff_lay = FeedForward(n_emb)\n",
        "        self.ln1 = nn.LayerNorm(n_emb)\n",
        "        self.ln2 = nn.LayerNorm(n_emb)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.mh_att(self.ln1(x))\n",
        "        x = x + self.ff_lay(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class TextGenerator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        #  x = [1, 25, 89, 65,63,64]\n",
        "        self.lookup_token_emd_table = nn.Embedding(vocab_size, n_emb)\n",
        "        self.postional_encoding = nn.Embedding(block_size, n_emb)\n",
        "        self.blocks = nn.Sequential(*[BlockSeq(n_emb, num_heads) for _ in range(n_x)])\n",
        "        self.layer_norm = nn.LayerNorm(n_emb)\n",
        "        self.model_head = nn.Linear(n_emb, vocab_size)\n",
        "\n",
        "    def forward(self, x, y=None): #\n",
        "        batches, block_size_x = x.shape\n",
        "        out = self.lookup_token_emd_table(x) # 2, 7, 90 , x: 1,2 3\n",
        "        pos_enc = self.postional_encoding(torch.arange(block_size_x, device=device))\n",
        "        out = out + pos_enc\n",
        "        out = self.blocks(out)\n",
        "        out = self.layer_norm(out)\n",
        "        out = self.model_head(out)\n",
        "\n",
        "\n",
        "        if y is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            batches, block_size, X = out.shape\n",
        "            loss = F.cross_entropy(out.view(batches*block_size, X), y.view(batches*block_size))\n",
        "\n",
        "        return out, loss\n",
        "\n",
        "    def generate(self, x, max_tokens=200):\n",
        "        for _ in range(max_tokens):\n",
        "            logits, _ = self(x[:, -block_size:])\n",
        "            logits = logits[:, -1, :]\n",
        "            # print(logits.shape)\n",
        "            probilities = F.softmax(logits, dim=-1) # 1, 90\n",
        "            next_x = torch.multinomial(probilities, num_samples=1)\n",
        "            x = torch.cat((x, next_x), dim=1) # [hi, ] 1 2 3\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "ZWfw_aBJqcfM"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batches(split):\n",
        "    data = train if split == 'train' else test\n",
        "    #\n",
        "    indexes = torch.randint(len(data) - block_size, (batch_size, ))\n",
        "    # print(indexes)\n",
        "    x = torch.stack([data[i:i+block_size] for i in indexes])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in indexes])\n",
        "        # data[4020066: 4020066+block]\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n"
      ],
      "metadata": {
        "id": "bfaJC7jGsHwh"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def losses_estimate():\n",
        "  output = {}\n",
        "  model.eval()\n",
        "  for split in ['train', 'test']:\n",
        "    losses = torch.zeros(eval_iteration)\n",
        "    for i in range(eval_iteration):\n",
        "      x, y = batches(split)\n",
        "      logits, loss = model(x, y)\n",
        "      losses[i] = loss.item()\n",
        "    output[split] = losses.mean()\n",
        "  model.train()\n",
        "  return output"
      ],
      "metadata": {
        "id": "YuuvEeRvr7Qy"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "meta = {\n",
        "    'vocab_size': vocab_size,\n",
        "    'itos': int_to_str,\n",
        "    'stoi': str_to_int,\n",
        "}\n",
        "with open('meta.pkl', 'wb') as f:\n",
        "    pickle.dump(meta, f)"
      ],
      "metadata": {
        "id": "23UrCnXcxqCn"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = TextGenerator()\n",
        "model = model.to(device)\n",
        "print(sum(p.numel() for p in model.parameters())/1e6, 'Millions parameters')\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=6e-4)\n",
        "for iter in range(max_iters):\n",
        "  if iter % eval_iteration == 0 :\n",
        "    losses = losses_estimate()\n",
        "    print(f\"Iteration no: {iter}, training loss: {losses['train']:.3f}  and val loss: {losses['test']:.3f}\")\n",
        "\n",
        "  x, y = batches('train')\n",
        "  logits, loss = model(x, y)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCeQP34mrtIY",
        "outputId": "23437865-6a8c-48dd-8e94-a52255e5cfb7"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.213722 Millions parameters\n",
            "Iteration no: 0, training loss: 4.562  and val loss: 4.583\n",
            "Iteration no: 250, training loss: 2.371  and val loss: 2.547\n",
            "Iteration no: 500, training loss: 2.182  and val loss: 2.406\n",
            "Iteration no: 750, training loss: 2.072  and val loss: 2.323\n",
            "Iteration no: 1000, training loss: 1.983  and val loss: 2.242\n",
            "Iteration no: 1250, training loss: 1.931  and val loss: 2.202\n",
            "Iteration no: 1500, training loss: 1.884  and val loss: 2.148\n",
            "Iteration no: 1750, training loss: 1.845  and val loss: 2.129\n",
            "Iteration no: 2000, training loss: 1.818  and val loss: 2.109\n",
            "Iteration no: 2250, training loss: 1.798  and val loss: 2.087\n",
            "Iteration no: 2500, training loss: 1.763  and val loss: 2.049\n",
            "Iteration no: 2750, training loss: 1.752  and val loss: 2.050\n",
            "Iteration no: 3000, training loss: 1.727  and val loss: 2.027\n",
            "Iteration no: 3250, training loss: 1.707  and val loss: 2.014\n",
            "Iteration no: 3500, training loss: 1.704  and val loss: 2.011\n",
            "Iteration no: 3750, training loss: 1.690  and val loss: 2.003\n",
            "Iteration no: 4000, training loss: 1.673  and val loss: 1.990\n",
            "Iteration no: 4250, training loss: 1.658  and val loss: 1.980\n",
            "Iteration no: 4500, training loss: 1.642  and val loss: 1.963\n",
            "Iteration no: 4750, training loss: 1.638  and val loss: 1.968\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.save(model, 'entire_model.pth') # save the model"
      ],
      "metadata": {
        "id": "9a16wYZYqv9C"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('meta.pkl', 'rb') as f:\n",
        "    meta = pickle.load(f)\n",
        "stoi, itos = meta['stoi'], meta['itos']\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: [itos[i] for i in l]"
      ],
      "metadata": {
        "id": "HKdXi4dRqz0d"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reply(message, history):\n",
        "    # encode the beginning of the prompt\n",
        "    start = message\n",
        "    start_ids = encode(start)\n",
        "    x = (torch.tensor(start_ids, dtype=torch.long, device='cuda')[None, ...])\n",
        "    print(x)\n",
        "    replied = []\n",
        "    # run generation\n",
        "    with torch.no_grad():\n",
        "        for k in range(3):\n",
        "            y = model.generate(x, 20)\n",
        "            replied.append(''.join(decode(y[0].tolist())))\n",
        "    return '\\n'.join(replied)"
      ],
      "metadata": {
        "id": "QU1QMj_Fq4ZI"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gradio_client import Client\n",
        "\n",
        "client = Client(\"https://4072dbe68514fc9d1d.gradio.live/\")\n",
        "try:\n",
        "    result = client.predict(message=\"Hello!!\", api_name=\"/predict\")\n",
        "    print(result)\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFrsrKlZ3Gfg",
        "outputId": "63ab1b69-7b05-4c6c-e5f8-67c1aa190f9f"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded as API: https://4072dbe68514fc9d1d.gradio.live/ ✔\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 624, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/route_utils.py\", line 323, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 2015, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1562, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/to_thread.py\", line 33, in run_sync\n",
            "    return await get_asynclib().run_sync_in_worker_thread(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 865, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "  File \"<ipython-input-55-0ca978c1e8a6>\", line 5, in reply\n",
            "    x = (torch.tensor(start_ids, dtype=torch.long, device='cuda')[None, ...])\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\", line 319, in _lazy_init\n",
            "    torch._C._cuda_init()\n",
            "RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An error occurred: The upstream Gradio app has raised an exception but has not enabled verbose error reporting. To enable, set show_error=True in launch().\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def greet(name, intensity):\n",
        "    return \"Hello, \" + name + \"!\" * int(intensity)\n",
        "\n",
        "demo = gr.Interface(\n",
        "    fn=greet,\n",
        "    inputs=[\"text\", \"slider\"],\n",
        "    outputs=[\"text\"],\n",
        ")\n",
        "\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "id": "78vm3etg5vnJ",
        "outputId": "00d46f69-60fd-4094-8921-d06cc9d16f80"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://bfc96709b29198af37.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://bfc96709b29198af37.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def greet(name):\n",
        "    return \"Hello \" + name + \"!\"\n",
        "\n",
        "demo = gr.Interface(fn=greet, inputs=\"textbox\", outputs=\"textbox\")\n",
        "\n",
        "demo.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "id": "oQQt87A46h5l",
        "outputId": "85140d58-18a7-4fb6-8285-eb69ea6c7fd0"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://ab10d7bab6f4ab53c4.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://ab10d7bab6f4ab53c4.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    }
  ]
}